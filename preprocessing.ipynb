{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dialogs.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r', encoding = 'utf-8') as file:\n",
    "    lines = file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_docs = []\n",
    "target_docs = []\n",
    "\n",
    "input_tokens = set()\n",
    "target_tokens = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    input_doc, target_doc = line.split('\\t')\n",
    "    input_docs.append(input_doc)\n",
    "    \n",
    "    target_doc = \" \".join(re.findall(r\"[\\w']+|['^\\w\\s]\", target_doc))\n",
    "    target_doc = '<START> ' + target_doc + ' <END>'\n",
    "    target_docs.append(target_doc)\n",
    "    \n",
    "    for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "        if token not in input_tokens:\n",
    "            input_tokens.add(token)\n",
    "    for token in target_doc.split():\n",
    "        if token not in target_tokens:\n",
    "            target_tokens.add(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\w\\s]\", input_doc)) for input_doc in input_docs])\n",
    "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\w\\s]\", target_doc)) for target_doc in target_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features_dict = dict(\n",
    "    [(token, index) for index, token in enumerate(input_tokens)]\n",
    ")\n",
    "\n",
    "target_features_dict = dict(\n",
    "    [(token, index) for index, token in enumerate(target_tokens)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_features_dict = dict(\n",
    "    [(index, token) for token, index in input_features_dict.items()]\n",
    ")\n",
    "\n",
    "reverse_target_features_dict = dict(\n",
    "    [(index, token ) for token, index  in target_features_dict.items()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens), \n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_docs), max_encoder_seq_length, num_decoder_tokens)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "\n",
    "  for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
    "      \n",
    "    encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n",
    "\n",
    "  for timestep, token in enumerate(target_doc.split()):\n",
    "\n",
    "    decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n",
    "    if timestep > 0:\n",
    "\n",
    "      decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cloth', 450), ('club', 451), ('coat', 452), ('code', 453), ('coffee', 454), ('coffin', 455), ('cold', 456), ('colder', 457), ('college', 458), ('collision', 459), ('color', 460), ('come', 461), ('comes', 462), ('comfortable', 463), ('coming', 464), ('commercials', 465), ('common', 466), ('company', 467), ('complain', 468), ('complained', 469), ('complainers', 470), ('complaining', 471), ('completely', 472), ('computer', 473), ('computers', 474), ('concrete', 475), ('conditioner', 476), ('cone', 477), ('cones', 478), ('conference', 479), ('congratulations', 480), ('considering', 481), ('constantly', 482), ('continued', 483), ('controls', 484), ('conversing', 485), ('converter', 486), ('cook', 487), ('cooking', 488), (\"cooks'\", 489), ('cool', 490), ('cop', 491), ('cops', 492), ('corner', 493), ('corporations', 494), ('correct', 495), ('cost', 496), ('costs', 497), ('coughing', 498), ('could', 499)]\n",
      "[(450, 'climbing'), (451, 'clock'), (452, 'close'), (453, 'closed'), (454, 'closer'), (455, 'cloth'), (456, 'clothes'), (457, 'club'), (458, 'coat'), (459, 'code'), (460, 'coffee'), (461, 'coffin'), (462, 'cold'), (463, 'colder'), (464, 'college'), (465, 'collision'), (466, 'color'), (467, 'com'), (468, 'come'), (469, 'comes'), (470, 'comfortable'), (471, 'coming'), (472, 'commercials'), (473, 'common'), (474, 'company'), (475, 'complain'), (476, 'complained'), (477, 'complainers'), (478, 'complaining'), (479, 'completely'), (480, 'computer'), (481, 'computers'), (482, 'concrete'), (483, 'conditioner'), (484, 'cone'), (485, 'cones'), (486, 'congratulations'), (487, 'considering'), (488, 'constantly'), (489, 'continued'), (490, 'controls'), (491, 'converter'), (492, 'cook'), (493, 'cooking'), (494, \"cooks'\"), (495, 'cool'), (496, 'cop'), (497, 'cops'), (498, 'corner'), (499, 'corporations')]\n",
      "['hot', 'hotel', 'hotter', 'hour', 'hours', 'house', 'housekeeping', 'houses', 'how', \"how's\", 'hp', 'hug', 'huge', 'human', 'hunch', 'hundred', 'hungry', 'hurricanes', 'hurry', 'hurt', 'hurts', 'husband', 'hysterically', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ice', 'idea', 'ideas', 'identify', 'if', 'ii', 'ill', 'imagine', 'immediately', 'important', 'impossible', 'improve', 'in', 'inch', 'incomplete', 'increase', 'initial', 'injured', 'ink', 'insert', 'inside', 'inspection', 'installation', 'instance', 'instantly', 'instead', 'instructions', 'instruments', 'insurance', 'intend', 'intense', 'interested', 'interesting', 'interests', 'internet', 'intersection', 'interview', 'into', 'invading', 'invented', 'invitation', 'invitations', 'invite', 'invited', 'invites', 'inviting', 'ipod', 'iq', 'iron', 'is', 'island', \"isn't\", 'it', \"it'll\", \"it's\", 'its', 'jacket', 'jail', 'jammed', 'january', 'jar', 'jaywalking', 'jazz', 'jerk', 'jerks', \"jessica's\", 'jet', 'jets', 'job', 'jobs', 'jogging', 'joining', 'joke', 'jokes', 'joking', 'judge', 'judy', 'juice', 'june', 'just', 'keep', 'kept', 'keys', 'kid', 'kidding', 'kids', 'kill', 'killed', 'killer', 'killing', 'kind', 'kinds', 'king', 'kitchen', 'kittens', 'knee', 'knew', 'knife', 'knitting', 'knock', 'knocking', 'know', 'known', 'knows', 'labor', 'lady', 'laid', 'lake', 'land', 'landed', 'lane', 'language', 'languages', 'laptop', 'large', 'las', 'last', 'lasts', 'late', 'lately', 'later', 'latest', 'laugh', 'laughed', 'laughing', 'laundry', 'law', 'lawns', 'lawsuits', 'layoffs', 'lead', 'leaders', 'league', 'learn', 'learned', 'learning', 'least', 'leather', 'leave', 'leaves', 'left', 'leftovers', 'legislators', 'legs', 'lemon', 'lend', 'lesson', 'lessons', 'let', \"let's\", 'letter', 'lettuce', 'liar', 'library', 'license', 'licks', 'lie', 'life', 'lifeguard', 'light', 'lighters', 'like', 'liked', 'likes', 'limit', 'line', 'lips', 'list', 'listen', 'listening', 'little', 'live']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print out those value here:\n",
    "print\n",
    "print(list(input_features_dict.items())[450:500])\n",
    "# print(list(input_features_dict.values())[450:500])\n",
    "print(list(reverse_target_features_dict.items())[450:500])\n",
    "# print(list(reverse_target_features_dict.values())[40:90])\n",
    "print(list(input_tokens)[1000:1200])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mycondaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
